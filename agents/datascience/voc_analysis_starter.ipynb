{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice-of-Customer Data Science Analysis Starter\n",
    "\n",
    "This notebook provides a starting point for analyzing Voice-of-Customer data collected by the platform. It demonstrates how to:\n",
    "\n",
    "1. Connect to Astra DB to retrieve data\n",
    "2. Analyze sentiment trends over time\n",
    "3. Perform topic modeling on customer feedback\n",
    "4. Create visualizations for insights\n",
    "5. Publish insights back to the platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "import duckdb\n",
    "import requests\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure environment\n",
    "TENANT_ID = os.getenv(\"TENANT_ID\", \"default\")\n",
    "TENANT_NAME = os.getenv(\"TENANT_NAME\", \"Default Tenant\")\n",
    "ASTRA_DB_ID = os.getenv(\"ASTRA_DB_ID\")\n",
    "ASTRA_DB_REGION = os.getenv(\"ASTRA_DB_REGION\")\n",
    "ASTRA_TOKEN = os.getenv(\"ASTRA_TOKEN\")\n",
    "ASTRA_KEYSPACE = os.getenv(\"ASTRA_KEYSPACE\", \"voc_platform\")\n",
    "MCP_SERVER_URL = os.getenv(\"MCP_SERVER_URL\", \"http://mcp-server:3000\")\n",
    "\n",
    "print(f\"Analysis for tenant: {TENANT_NAME} ({TENANT_ID})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Astra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to connect to Astra DB\n",
    "def connect_to_astra():\n",
    "    # Create a secure connect bundle path\n",
    "    # In a production environment, download the bundle from Astra DB dashboard\n",
    "    # For this starter, we'll use the MCP Server as a proxy to Astra DB\n",
    "    \n",
    "    cloud_config = {\n",
    "        'secure_connect_bundle': 'path/to/secure-connect-bundle.zip'\n",
    "    }\n",
    "    \n",
    "    auth_provider = PlainTextAuthProvider(\n",
    "        'token', \n",
    "        ASTRA_TOKEN\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # For local testing, we'll use the MCP server\n",
    "        print(\"Using MCP server for data access\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Astra DB: {e}\")\n",
    "        return None\n",
    "\n",
    "# Alternative function to get data via MCP server API\n",
    "def get_data_via_mcp(endpoint, params=None):\n",
    "    url = f\"{MCP_SERVER_URL}/api/{endpoint}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {ASTRA_TOKEN}\"\n",
    "    }\n",
    "    \n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    # Add tenant_id if not present\n",
    "    if 'tenant_id' not in params:\n",
    "        params['tenant_id'] = TENANT_ID\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling MCP API: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get raw documents\n",
    "def get_raw_docs(limit=1000, source_type=None, days_back=30):\n",
    "    params = {\n",
    "        \"limit\": limit,\n",
    "        \"days\": days_back\n",
    "    }\n",
    "    \n",
    "    if source_type:\n",
    "        params[\"source_type\"] = source_type\n",
    "    \n",
    "    result = get_data_via_mcp(\"raw-docs/list\", params)\n",
    "    \n",
    "    if result and 'documents' in result:\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(result['documents'])\n",
    "        \n",
    "        # Parse timestamps\n",
    "        if 'created_at' in df.columns:\n",
    "            df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        \n",
    "        # Extract metadata fields\n",
    "        if 'metadata' in df.columns:\n",
    "            # Extract sentiment and topics if they exist\n",
    "            df['sentiment'] = df['metadata'].apply(lambda x: x.get('sentiment', 'neutral') if x else 'neutral')\n",
    "            df['urgency'] = df['metadata'].apply(lambda x: x.get('urgency', False) if x else False)\n",
    "            df['topics'] = df['metadata'].apply(lambda x: x.get('topics', []) if x else [])\n",
    "            df['brands'] = df['metadata'].apply(lambda x: x.get('brands', []) if x else [])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Get sample data\n",
    "docs_df = get_raw_docs(limit=100)\n",
    "\n",
    "# Display basic statistics\n",
    "if not docs_df.empty:\n",
    "    print(f\"Retrieved {len(docs_df)} documents\")\n",
    "    if 'source_type' in docs_df.columns:\n",
    "        print(\"\\nSource type distribution:\")\n",
    "        print(docs_df['source_type'].value_counts())\n",
    "    \n",
    "    if 'sentiment' in docs_df.columns:\n",
    "        print(\"\\nSentiment distribution:\")\n",
    "        print(docs_df['sentiment'].value_counts())\n",
    "else:\n",
    "    print(\"No documents found or error retrieving data\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    print(\"Creating sample data for demonstration purposes\")\n",
    "    \n",
    "    # Sample data generation\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(end=datetime.now(), periods=100, freq='D')\n",
    "    \n",
    "    sentiments = np.random.choice(['positive', 'neutral', 'negative'], size=100, p=[0.3, 0.5, 0.2])\n",
    "    \n",
    "    source_types = np.random.choice(['website', 'review_site', 'social_media', 'serp'], size=100, p=[0.4, 0.3, 0.2, 0.1])\n",
    "    \n",
    "    topics_list = [\n",
    "        ['shipping', 'delivery'], \n",
    "        ['price', 'cost'], \n",
    "        ['quality', 'durability'],\n",
    "        ['customer service', 'support'],\n",
    "        ['ease of use', 'user interface'],\n",
    "        ['features', 'functionality'],\n",
    "        ['reliability', 'uptime'],\n",
    "        ['performance', 'speed']\n",
    "    ]\n",
    "    \n",
    "    topics = [np.random.choice(topics_list, size=np.random.randint(1, 3)).tolist() for _ in range(100)]\n",
    "    \n",
    "    brands = [['Brand A', 'Brand B'] if i % 5 == 0 else ['Brand A'] if i % 3 == 0 else ['Brand C'] for i in range(100)]\n",
    "    \n",
    "    urgency = np.random.choice([True, False], size=100, p=[0.15, 0.85])\n",
    "    \n",
    "    docs_df = pd.DataFrame({\n",
    "        'doc_id': [str(uuid.uuid4()) for _ in range(100)],\n",
    "        'tenant_id': TENANT_ID,\n",
    "        'source_type': source_types,\n",
    "        'created_at': dates,\n",
    "        'sentiment': sentiments,\n",
    "        'urgency': urgency,\n",
    "        'topics': topics,\n",
    "        'brands': brands,\n",
    "        'content': ['Sample content ' + str(i) for i in range(100)]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nSample data source type distribution:\")\n",
    "    print(docs_df['source_type'].value_counts())\n",
    "    \n",
    "    print(\"\\nSample data sentiment distribution:\")\n",
    "    print(docs_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Time series sentiment analysis\n",
    "def plot_sentiment_over_time(df):\n",
    "    if df.empty or 'created_at' not in df.columns or 'sentiment' not in df.columns:\n",
    "        print(\"Cannot create time series: Missing required columns\")\n",
    "        return\n",
    "    \n",
    "    # Resample by day and count sentiments\n",
    "    sentiment_counts = df.set_index('created_at').\\\n",
    "        groupby([pd.Grouper(freq='D'), 'sentiment']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Ensure all sentiment columns exist\n",
    "    for sentiment in ['positive', 'neutral', 'negative']:\n",
    "        if sentiment not in sentiment_counts.columns:\n",
    "            sentiment_counts[sentiment] = 0\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    window = 7\n",
    "    rolling_sentiments = sentiment_counts.rolling(window=window).mean()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Plot raw sentiment counts\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sentiment_counts.plot(ax=plt.gca(), kind='bar', stacked=True, \n",
    "                          color={'positive': 'green', 'neutral': 'grey', 'negative': 'red'})\n",
    "    plt.title(f'Daily Sentiment Distribution for {TENANT_NAME}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Sentiment')\n",
    "    \n",
    "    # Plot rolling averages\n",
    "    plt.subplot(2, 1, 2)\n",
    "    rolling_sentiments.plot(ax=plt.gca(), kind='line', \n",
    "                           color={'positive': 'green', 'neutral': 'grey', 'negative': 'red'})\n",
    "    plt.title(f'{window}-Day Rolling Average Sentiment')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Average Count')\n",
    "    plt.legend(title='Sentiment')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot sentiment over time\n",
    "plot_sentiment_over_time(docs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract and analyze topics\n",
    "def analyze_topics(df):\n",
    "    if df.empty or 'topics' not in df.columns:\n",
    "        print(\"Cannot analyze topics: Missing required column\")\n",
    "        return\n",
    "    \n",
    "    # Flatten topics list\n",
    "    all_topics = []\n",
    "    for topics_list in df['topics']:\n",
    "        if isinstance(topics_list, list):\n",
    "            all_topics.extend(topics_list)\n",
    "        elif isinstance(topics_list, str):\n",
    "            # Handle case where topics might be a string representation of a list\n",
    "            try:\n",
    "                parsed = json.loads(topics_list.replace(\"'\", \"\\\"\"))\n",
    "                if isinstance(parsed, list):\n",
    "                    all_topics.extend(parsed)\n",
    "            except:\n",
    "                all_topics.append(topics_list)\n",
    "    \n",
    "    # Count topic frequencies\n",
    "    topic_counts = pd.Series(all_topics).value_counts()\n",
    "    \n",
    "    # Plot top topics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    topic_counts.head(15).plot(kind='barh', color='skyblue')\n",
    "    plt.title(f'Top 15 Topics Mentioned - {TENANT_NAME}')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Topic')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze topics\n",
    "analyze_topics(docs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze data by source\n",
    "def analyze_by_source(df):\n",
    "    if df.empty or 'source_type' not in df.columns:\n",
    "        print(\"Cannot analyze by source: Missing required column\")\n",
    "        return\n",
    "    \n",
    "    # Count by source type\n",
    "    source_counts = df['source_type'].value_counts()\n",
    "    \n",
    "    # Plot source distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    source_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90, cmap='Pastel1')\n",
    "    plt.title(f'Distribution of Data Sources - {TENANT_NAME}')\n",
    "    plt.ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze by source\n",
    "analyze_by_source(docs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publishing Insights to Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to publish an insight to the platform\n",
    "def publish_insight(title, description, figure=None, source_ids=None, metadata=None):\n",
    "    \"\"\"Publish an insight to the platform.\"\"\"\n",
    "    \n",
    "    # Create insight data\n",
    "    insight_data = {\n",
    "        \"tenant_id\": TENANT_ID,\n",
    "        \"insight_id\": str(uuid.uuid4()),\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"source_ids\": source_ids or [],\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"metadata\": metadata or {}\n",
    "    }\n",
    "    \n",
    "    # Save figure if provided\n",
    "    if figure is not None:\n",
    "        # In a production environment, this would save to S3/MinIO\n",
    "        # For this demo, we'll just note it\n",
    "        print(f\"Would save figure for insight: {title}\")\n",
    "        \n",
    "    # Send insight to MCP server\n",
    "    result = get_data_via_mcp(\"insights/create\", insight_data)\n",
    "    \n",
    "    if result and result.get(\"success\"):\n",
    "        print(f\"✅ Published insight: {title}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"❌ Failed to publish insight: {title}\")\n",
    "        return False\n",
    "\n",
    "# Example: Publish a sample insight\n",
    "# publish_insight(\n",
    "#     title=\"Sentiment Trend Analysis: Q1 2024\",\n",
    "#     description=\"Analysis of customer sentiment trends over Q1 2024 shows a 15% increase in positive sentiment related to product quality, while concerns about shipping times have decreased by 8%.\",\n",
    "#     metadata={\n",
    "#         \"period\": \"Q1 2024\",\n",
    "#         \"key_findings\": [\"Positive sentiment increase\", \"Shipping concerns decreased\"],\n",
    "#         \"recommendation\": \"Continue product quality improvements\"\n",
    "#     }\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
